
<!DOCTYPE html>
<html>

<head lang="en">
    <meta charset="UTF-8">
    <meta http-equiv="x-ua-compatible" content="ie=edge">

    <title>NerVE</title>

    <meta name="description" content="">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel='icon' href='/icons/icon-bulb-2-512.png' sizes='512x512'>

    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/css/bootstrap.min.css">
    <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/font-awesome/4.4.0/css/font-awesome.min.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.css">
    <link rel="stylesheet" href="css/app.css">

    <link rel="stylesheet" href="css/bootstrap.min.css">

    <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js"></script>
    <script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.5/js/bootstrap.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/codemirror/5.8.0/codemirror.min.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.5.3/clipboard.min.js"></script>


    
    <script src="js/app.js"></script>
</head>

<body>
    <div class="container" id="main">
        <div class="row">
            <h1 class="col-md-12 text-center">
                <b>NerVE: Neural Volumetric Edges for Parametric Curve <br> Extraction from Point Cloud <br>
                <small>
                    CVPR 2023
                </small>
            </h1>
        </div>
        <div class="row">
            <div class="col-md-12 text-center">
                <ul class="list-inline">
                    <li>
                        <!-- <a href="https://lingtengqiu.github.io/"> -->
                            Xiangyu Zhu
                        </a><sup>1*</sup>
                    </li>
                    <li>
                        <a href="https://dongdu3.github.io/">
                            Dong Du
                        </a><sup>1*</sup>
                    </li>
                    <li>
                        <a href="http://chenweikai.github.io/">
                            Weikai Chen
                        </a><sup>2</sup>
                    </li><br>
                    <li>
                            Zhiyou Zhao
                        </a><sup>1</sup>
                    </li>
                    <li>
                        <a href="https://yinyunie.github.io/">
                            Yinyu Nie
                        </a><sup>3</sup>
                    </li>
                    <li>
                        <a href="https://gaplab.cuhk.edu.cn/">
                            Xiaoguang Han
                        </a><sup>1#</sup>
                    </li>
                    <!-- Inc. -->
                    <br>
                    <li>
                        <small>
                            <sup>1</sup>The Chinese University of Hong Kong, Shenzhen
                        </small>
                    </li>
                    <li>
                        <small>
                            <sup>2</sup>Tencent America
                        </small>
                        
                    </li>
                    <li>
                        <small><sup>3</sup>Technical University of Munich
                        
                    </li>
                </ul>
            </div>
        </div>



        <div class="row">
                <div class="col-md-4 col-md-offset-4 text-center">
                    <ul class="nav nav-pills nav-justified">
                        <li>
                            <a href="">
                            <image src="./icons/paper.png" height="60px">
                                <h4><strong>Paper</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="">
                            <image src="./icons/youtube.png" height="60px">
                                <h4><strong>Video</strong></h4>
                            </a>
                        </li>
                        <li>
                            <a href="https://github.com/dongdu3/NerVE">
                            <image src="./icons/github.png" height="60px">
                                <h4><strong>Code</strong></h4>
                            </a>
                        </li>
                    </ul>
                </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Abstract
                </h3>
                <image src="./images/teaser.png" class="img-responsive" alt="overview"><br>
                <p class="text-justify">
                    Extracting parametric edge curves from point clouds is a fundamental problem in 3D vision and geometry processing. 
                    Existing approaches mainly rely on keypoint detection, a challenging procedure that tends to generate noisy output, making the subsequent edge extraction error-prone.
                    To address this issue, we propose to directly detect structured edges to circumvent the limitations of the previous point-wise methods.
                    We achieve this goal by presenting NerVE, a novel neural volumetric edge representation that can be easily learned through a volumetric learning framework.
                    NerVE can be seamlessly converted to a versatile piece-wise linear (PWL) curve representation, enabling a unified strategy for learning all types of free-form curves. 
                    Furthermore, as NerVE encodes rich structural information, we show that edge extraction based on NerVE can be reduced to a simple graph search problem.
                    After converting NerVE to the PWL representation, parametric curves can be obtained via off-the-shelf spline fitting algorithms.We evaluate our method on the challenging ABC dataset.
                    We show that a simple network based on NerVE can already outperform the previous state-of-the-art methods by a great margin.                           
                </p><br>
            </div>
        </div>



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Video
                </h3>
                <div class="text-center">
                    <div style="position:relative;padding-top:56.25%;">
                        <iframe src="./media/demo.mp4" allowfullscreen style="position:absolute;top:0;left:0;width:100%;height:100%;"></iframe>
                    </div>
                </div>
        </div> -->



        <!-- <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Method
                </h3>
                <p class="text-justify">
                    Given a monocular video with Ni frames depicting a moving person {It|t = 1, . . . , Ni}, REC-MV aims to reconstruct high-fidelity and space-time coherent open garment meshes. This is a challenging problem as it requires a
                    method to simultaneously capture the shape contours, local surface details, and the motion of the garment. Observing that feature curves (e.g., necklines, hemlines) provide critical cues for 
                    determining the shape contours of garment and implicit signed distance function (SDF) can well represent a detailed closed surface, we propose to first optimize the explicit 3D feature curves and
                    implicit garment surfaces from the video, and then apply non-rigid clothing template registration to extract the open garment meshes.
                </p><br>
                <image src="./images/pipeline.png" class="img-responsive" alt="method"><br>
                <p class="text-justify">
                    Our method can be devided into 4 parts:<br>
                    (a) Starting from a surface template, we initialize the canonical curves by solving Eq. (3), and apply a handle-based deformation to initialize the canonical implicit surface.<br>
                    (b) Given an i-th frame, canonical curves are deformed to the camera view space to compute the projection loss based on the surface-aware visibility estimation. <br>
                    (c) Similarly, the canonical surface  is deform to the camera view to compute the photometric loss by differentiable rendering. The curves and surface are jointly optimized to enable a progressive co-evolution.<br>
                    (d) Last, the open garment meshes can be extracted by template registration in the canonical space.
                </p><br>
            </div>
        </div>



        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Qualitative Results
                </h3>
                <image src="./images/qualitative_results(a).png" class="img-responsive" alt="overview"><br>
                <image src="./images/qualitative_results(b).png" class="img-responsive" alt="overview"><br>
                <image src="./images/qualitative_results(e).png" class="img-responsive" alt="overview"><br>
                <p class="text-center">
                    <strong>
                        Qualitative comparison on real datasets between BCNet, ClothWild, ReEF, and our method
                    </strong>
                </p>
                <image src="./images/qualitative_results(d).png" class="img-responsive" alt="overview"><br>
                <p class="text-center">
                    <strong>
                        Reconstruction results on a large pose sequence
                    </strong>
                </p><br>

                <image src="./images/qualitative_results(c).png" class="img-responsive" alt="overview"><br>
                <p class="text-center">
                    <strong>
                        Dynamic garment reconstruction results of our method
                    </strong>
                </p>

            </div>
        </div> -->

            
        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    BibTeX
                </h3>
                <div class="form-group col-md-10 col-md-offset-1">
                    <textarea id="bibtex" class="form-control" readonly>
                    @inproceedings{zhu2023NerVE,

                    year={2023}
                    }
                    </textarea>
                </div>
            </div>
        </div>

        <div class="row">
            <div class="col-md-8 col-md-offset-2">
                <h3>
                    Acknowledgements
                </h3>
                <p class="text-justify">
                    <!-- This work was partially supported by the National Key R&D Program of China (No.2018YFB1800800), the Basic Research Project No.~HZQB-KCZYZ-2021067 of Hetao Shenzhen-HK S&T Cooperation Zone, and Hong Kong RGC GRF grant (project# 17203119).
                    <br> -->
                    The website template was borrowed from <a href="http://mgharbi.com/">MichaÃ«l Gharbi</a>.
                </p>
            </div>
        </div>
    </div>
</body>
</html>